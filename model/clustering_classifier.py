import logging
import os
import sys
from threading import main_thread
import time
from torch import nn
import tqdm
import torch
import numpy as np
import argparse
import random
import datetime
from os.path import join
from easydict import EasyDict
from logging import getLogger
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import yaml
from resnet import resnet50
import tensorboardX
from sklearn_extra.cluster import KMedoids
import matplotlib.pyplot as plt
sys.path.append(join(os.path.dirname(os.path.abspath(__file__)), "../"))
from auxiliary.my_utils import plant_seeds, visuaize_pts
#from dataset.dataset_shapenet_views import ShapeNet
from dataset.dataset_shapenet_views_clustering import ShapeNet as ShapeNet_Cluster
from eval.metric import ChamferDistanceL2, compute_ptcloud_dismatrix_batch, get_cluster_centroid
import auxiliary.ChamferDistancePytorch.chamfer3D.dist_chamfer_3D as dist_chamfer_3D
from auxiliary.ChamferDistancePytorch.fscore import fscore

parser = argparse.ArgumentParser()
parser.add_argument("--base-dir", type=str, default='log/benchmark/20210202_2055_viewer_clustering_seed1', 
                    help='')
parser.add_argument("--dismat-fname", type=str, default='viewer_trainnv1_cls13True.npy', 
                    help='')   
parser.add_argument('--workers', type=int, help='number of data loading workers', default=8) 
parser.add_argument("--views_search", action="store_true", help="increment views to train")
parser.add_argument("--mode", type=str, default="viewer", choices=['viewer', 'object'])
parser.add_argument("--nviews_train", type=int, default=1, help='num of view per shape for training')
parser.add_argument("--nviews_test", type=int, default=1, help='num of view per shape for test')    

parser.add_argument('--normalization', type=str, default="UnitBall",
                    choices=['UnitBall', 'BoundingBox', 'Identity'])
parser.add_argument("--shapenet13", action="store_true", help="Load 13 usual shapenet categories")
parser.add_argument("--SVR", action="store_true", help="Single_view Reconstruction")
parser.add_argument("--sample", action="store_false", help="Sample the input pointclouds")
parser.add_argument('--class_choice', nargs='+', default=["table", "car"], type=str)
parser.add_argument('--number_points', type=int, default=2500, help='Number of point sampled on the object during training, and generated by atlasnet')
parser.add_argument('--number_points_eval', type=int, default=2500,
                    help='Number of points generated by atlasnet (rounded to the nearest squared number) ')
parser.add_argument("--manual_seed", type=int, default=1, help="if seed > 0, then fixed, if < 0 then random")
parser.add_argument("--img_aug", action="store_true", help="apply image augmentation like random crop")
parser.add_argument("--img_aug_type", type=str, default='rgb', choices=['grayscale', 'rgb', 'binary', 'color_aug', 'color_aug_random', 'autoaugment'])
parser.add_argument("--autoaug_type", type=str, default='ImageNet', choices=['ImageNet', 'CIFAR10', 'Seq', 'RGB', 'SVHN'])
parser.add_argument("--color_aug_factor", nargs='+', type=float, default=[1.0, 1.0, 1.0, 1.0], help='brightness, contrast, saturation, hue')
parser.add_argument("--mag_idx", default=0, type=int, help="magnitude level index")
parser.add_argument("--n_op", default=0, type=int, help="number of operations")
parser.add_argument("--prob", default=1, type=float, help="prob")
parser.add_argument("--demo", action="store_true", help="run demo autoencoder or single-view")
parser.add_argument('--no_compile_chamfer', action="store_true", help="compile c++ version chamfer distance")
parser.add_argument('--n_clusters', type=int, default=500, help="number of clusters")
parser.add_argument('--cluster_res_path', type=str, default=None, help="number of clusters")

parser.add_argument('--lrate', type=float, default=0.001, help='learning rate')
parser.add_argument('--nepoch', type=int, default=30, help='number of epochs to train for')
parser.add_argument('--decay_step', type=int, default=10, help='number of epochs to train for')
parser.add_argument('--batch_size', type=int, default=32, help='input batch size')
parser.add_argument('--batch_size_test', type=int, default=32, help='input batch size')
parser.add_argument('--description', type=str, default="", help='descript this training')
parser.add_argument('--optimize_option', type=int, default=0, help='')
parser.add_argument('--pretrained', action='store_false', default=True, help='')
parser.add_argument('--test', action='store_true', default=False, help='')
parser.add_argument('--network_dir', type=str, default='210205_0221_viewer_optimize1_pretrainedTrue_decay30', help='')
parser.add_argument('--network', type=str, default='clustering', help='')



def main(opt):
    nviews_dic = {"train":opt.nviews_train, "test":opt.nviews_test}
    dismat = np.load(join(opt.base_dir, opt.dismat_fname))
    plant_seeds(opt.manual_seed)

    if not opt.cluster_res_path:
        kmedoids = KMedoids(n_clusters=opt.n_clusters, 
        random_state=opt.manual_seed, metric='precomputed', init='k-medoids++').fit(dismat)
        print(kmedoids.inertia_/dismat.shape[0])
        matrix_part = kmedoids.labels_
        print("label", len(matrix_part), max(matrix_part), min(matrix_part), matrix_part[:10])

        label_to_cd_index, label_to_cd_dis, label_to_index = get_cluster_centroid(dismat, matrix_part)
        print("cluster center", len(label_to_cd_index))

        cluster_res = {}
        cluster_res['label'] = matrix_part
        cluster_res['centroid'] = label_to_cd_index
        cluster_res['centroid_distance'] = label_to_cd_dis
        cluster_res['label_cluster_index'] = label_to_index
        np.save(join(opt.base_dir, f"cluster_res_{opt.n_clusters}.npy"), cluster_res)
        OUTPUT_DIM = opt.n_clusters
    else:
        cluster_res = np.load(opt.cluster_res_path, allow_pickle=True)
        cluster_res = cluster_res.item()
        matrix_part = cluster_res['label'] 
        label_to_cd_index = cluster_res['centroid'] 
        label_to_cd_dis = cluster_res['centroid_distance'] 
        label_to_index = cluster_res['label_cluster_index']
        opt.n_clusters = len(label_to_index)
        OUTPUT_DIM = len(label_to_index)
    
    opt.logger.info(f"Use pretrained model: {opt.pretrained} Num of Clusters: {len(label_to_index)}")
    model = resnet50(pretrained=opt.pretrained)
    IN_FEATURES = model.fc.in_features 
    #fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)
    #model.fc = fc
    model.fc = nn.Sequential(nn.Linear(IN_FEATURES, 1024),
                                 nn.ReLU(),
                                 nn.Linear(1024, OUTPUT_DIM))
    
    if opt.test:
        chamfer = ChamferDistanceL2().to(opt.device)
        distChamfer = dist_chamfer_3D.chamfer_3DDist()
        model_path = os.path.join(opt.network_dir, 'network.pt')
        model.load_state_dict(torch.load(model_path))
        opt.logger.info(f"Loading Network Parameters from {model_path}")
        train_dataset = ShapeNet_Cluster(opt, train=True, 
                    num_image_per_object=nviews_dic['train'], 
                    cls_labels=matrix_part, pretrained=opt.pretrained)

        test_dataset = ShapeNet_Cluster(opt, train=False, 
                    num_image_per_object=nviews_dic['test'], 
                    pretrained=opt.pretrained)

        test_loader = torch.utils.data.DataLoader(test_dataset, 
                                    batch_size=opt.batch_size, 
                                    shuffle=False, 
                                    num_workers=opt.workers)

        model = model.to(opt.device)
        model.eval()
        pred_loss = 0.0
        pred_fscore = 0.0
        prediction_set = []
        test_set = []
        with torch.no_grad():   
            for batch in tqdm.tqdm(test_loader, total=len(test_loader)):
                images, gt_points, cam_rotmat = batch['image'], batch['points'], batch['cam_rotmat']
                images = images.to(opt.device)
                if opt.mode == 'viewer':
                    gt_points = torch.bmm(gt_points, torch.transpose(cam_rotmat, 1, 2))
                prob = model(images)
                _, labels = prob.topk(1, 1)
                pred_list = []
                for label in labels:
                    pred_dic = train_dataset[label_to_cd_index[label.item()]]
                    pred_points, cam_rotmat = pred_dic['points'].unsqueeze(0), pred_dic['cam_rotmat'].unsqueeze(0)
                    
                    if opt.mode == 'viewer':
                        pred_points = torch.bmm(pred_points, torch.transpose(cam_rotmat, 1, 2))
                    pred_list.append(pred_points)

                pred_pts_set = torch.cat(pred_list, dim=0)
                gt_points = gt_points.to(opt.device)
                pred_pts_set = pred_pts_set.to(opt.device)
                pred_loss += chamfer(gt_points, pred_pts_set).item()
                dist1, dist2, idx1, idx2 = distChamfer(gt_points, pred_pts_set)
                loss_fscore, _, _ = fscore(dist1, dist2)
                loss_fscore = loss_fscore.mean()
                pred_fscore += loss_fscore.item()
                pred_pts_set = pred_pts_set.detach().cpu()
                gt_points = gt_points.detach().cpu()
                prediction_set.append(pred_pts_set)
                test_set.append(gt_points)

        pred_loss /= len(test_loader)
        pred_fscore /= len(test_loader)
        logger.info(f"chamfer: {pred_loss:.6f}, fscore: {pred_fscore:.6f}")
        prediction_set = torch.cat(prediction_set, dim=0)
        test_set = torch.cat(test_set, dim=0)
        np.save(os.path.join(opt.save_folder, 'prediction.npy'), prediction_set)
        np.save(os.path.join(opt.save_folder, 'gt.npy'), test_set)

    else:

        if opt.optimize_option == 0:
            optimizer = optim.Adam(model.parameters(), lr=opt.lrate)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.decay_step, gamma=0.1)
        
        elif opt.optimize_option == 1:
        # optimization 1
            params = [
                    {'params': model.conv1.parameters(), 'lr': opt.lrate / 10},
                    {'params': model.bn1.parameters(), 'lr': opt.lrate / 10},
                    {'params': model.layer1.parameters(), 'lr': opt.lrate / 8},
                    {'params': model.layer2.parameters(), 'lr': opt.lrate / 6},
                    {'params': model.layer3.parameters(), 'lr': opt.lrate / 4},
                    {'params': model.layer4.parameters(), 'lr': opt.lrate / 2},
                    {'params': model.fc.parameters()}
                    ]
            optimizer = optim.Adam(params, lr = opt.lrate)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.decay_step, gamma=0.1)
        
        elif opt.optimize_option == 2:
            # optimization 2
            for param in model.parameters():
                param.requires_grad = False
            for param in model.fc.parameters():
                param.requires_grad = True

            optimizer = optim.Adam(model.fc.parameters(), lr = opt.lrate)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.decay_step, gamma=0.1)
        

        criterion = nn.CrossEntropyLoss()
        model = model.to(opt.device)
        criterion = criterion.to(opt.device)
        train_dataset = ShapeNet_Cluster(opt, train=True, 
                        num_image_per_object=nviews_dic['train'], 
                        cls_labels=matrix_part,
                        pretrained=opt.pretrained)
        # subset_index = random.sample(range(len(train_dataset)), 1000)
        # train_dataset = torch.utils.data.Subset(train_dataset, subset_index)
        train_loader = torch.utils.data.DataLoader(train_dataset, 
                                    batch_size=opt.batch_size, 
                                    shuffle=True, 
                                    num_workers=opt.workers)

        for group in optimizer.param_groups:
            logger.info(f"learning rate: {group['lr']}")

        opt.iter = 0
        for epoch in range(opt.nepoch):
            start_time = time.time()
            train_loss, train_acc_1, train_acc_5 = train(opt, model, 
                    train_loader, optimizer, 
                    criterion)
            #valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)
            #torch.save(model.state_dict(), join(opt.save_folder, f'{opt.mode}_cluster_clfier_{opt.description}_seed{opt.manual_seed}_optim{opt.optimize_option}.pt'))
            torch.save(model.state_dict(), join(opt.save_folder, f'network.pt'))
            end_time = time.time()
            epoch_mins, epoch_secs = epoch_time(start_time, end_time)
            
            logger.info(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
            logger.info(f'\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ' \
                f'Train Acc @5: {train_acc_5*100:6.2f}%')
            
            opt.tbwriter.add_scalar('train/loss', train_loss, epoch + 1)
            opt.tbwriter.add_scalar('train/acc_1', train_acc_1, epoch + 1)
            opt.tbwriter.add_scalar('train/acc_5', train_acc_5, epoch + 1)
            for g in optimizer.param_groups:
                logger.info(f"learning rate: {g['lr']}")

            opt.tbwriter.add_scalar('train/lr', optimizer.param_groups[-1]['lr'], epoch + 1)
            scheduler.step()

        opt.tbwriter.close()
        # print(f'\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1*100:6.2f}% | ' \
        #     f'Valid Acc @5: {valid_acc_5*100:6.2f}%')

        opt.logger.info("Testing.....")
        chamfer = ChamferDistanceL2().to(opt.device)
        distChamfer = dist_chamfer_3D.chamfer_3DDist()
        train_dataset = ShapeNet_Cluster(opt, train=True, 
                    num_image_per_object=nviews_dic['train'], 
                    cls_labels=matrix_part, pretrained=opt.pretrained)

        test_dataset = ShapeNet_Cluster(opt, train=False, 
                    num_image_per_object=nviews_dic['test'], 
                    pretrained=opt.pretrained)

        test_loader = torch.utils.data.DataLoader(test_dataset, 
                                    batch_size=opt.batch_size, 
                                    shuffle=False, 
                                    num_workers=opt.workers)

        model.eval()
        pred_loss = 0.0
        pred_fscore = 0.0
        prediction_set = []
        test_set = []
        with torch.no_grad():   
            for batch in tqdm.tqdm(test_loader, total=len(test_loader)):
                images, gt_points, cam_rotmat = batch['image'], batch['points'], batch['cam_rotmat']
                images = images.to(opt.device)
                if opt.mode == 'viewer':
                    gt_points = torch.bmm(gt_points, torch.transpose(cam_rotmat, 1, 2))
                prob = model(images)
                _, labels = prob.topk(1, 1)
                pred_list = []
                for label in labels:
                    pred_dic = train_dataset[label_to_cd_index[label.item()]]
                    pred_points, cam_rotmat = pred_dic['points'].unsqueeze(0), pred_dic['cam_rotmat'].unsqueeze(0)
                    
                    if opt.mode == 'viewer':
                        pred_points = torch.bmm(pred_points, torch.transpose(cam_rotmat, 1, 2))
                    pred_list.append(pred_points)

                pred_pts_set = torch.cat(pred_list, dim=0)
                gt_points = gt_points.to(opt.device)
                pred_pts_set = pred_pts_set.to(opt.device)
                pred_loss += chamfer(gt_points, pred_pts_set).item()
                dist1, dist2, idx1, idx2 = distChamfer(gt_points, pred_pts_set)
                loss_fscore, _, _ = fscore(dist1, dist2)
                loss_fscore = loss_fscore.mean()
                pred_fscore += loss_fscore.item()
                pred_pts_set = pred_pts_set.detach().cpu()
                gt_points = gt_points.detach().cpu()
                prediction_set.append(pred_pts_set)
                test_set.append(gt_points)

        pred_loss /= len(test_loader)
        pred_fscore /= len(test_loader)
        logger.info(f"chamfer: {pred_loss:.6f}, fscore: {pred_fscore:.6f}")
        prediction_set = torch.cat(prediction_set, dim=0)
        test_set = torch.cat(test_set, dim=0)
        np.save(os.path.join(opt.save_folder, 'prediction.npy'), prediction_set)
        np.save(os.path.join(opt.save_folder, 'gt.npy'), test_set)
    
def train(opt, model, iterator, optimizer, criterion):
    
    epoch_loss = 0
    epoch_acc_1 = 0
    epoch_acc_5 = 0
    model.train()
    for idx, dict in enumerate(iterator):
        x = dict['image']
        y = dict['label']

        x = x.to(opt.device)
        y = y.to(opt.device)
        optimizer.zero_grad()    
        y_pred = model(x)
        loss = criterion(y_pred, y)
        
        acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc_1 += acc_1.item()
        epoch_acc_5 += acc_5.item()

        opt.tbwriter.add_scalar('train/loss_iter', loss.item(), opt.iter)
        opt.iter += 1

    epoch_loss /= len(iterator)
    epoch_acc_1 /= len(iterator)
    epoch_acc_5 /= len(iterator)
        
    return epoch_loss, epoch_acc_1, epoch_acc_5


def print_iteration_stats(start_train_time, epoch, iteration, len_iter, 
                            nepoch, loss, acc_1, logger):
    """
    print stats at each iteration
    """
    current_time = time.time()
    ellpased_time = current_time - start_train_time
    total_time_estimated = nepoch * (len_iter) * ellpased_time / (
            0.00001 + iteration + 1.0 * epoch * len_iter)  # regle de 3
    ETL = total_time_estimated - ellpased_time
    logger.info(
        f"["
        + f"{epoch}"
        + f": "
        + f"{iteration}"
        + "/"
        + f"{len_iter}"
        + "] Train loss:  "
        + f"{loss.item() :3f}, Acc: {acc_1.item() :3f}   "
        + f"Ellapsed Time: {ellpased_time / 60 :3f}min "
        + f"ETL: {ETL / 60 :3f} min")


def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

def calculate_topk_accuracy(y_pred, y, k = 5):
    with torch.no_grad():
        batch_size = y.shape[0]
        _, top_pred = y_pred.topk(k, 1)
        top_pred = top_pred.t()
        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))
        correct_1 = correct[:1].view(-1).float().sum(0, keepdim = True)
        correct_k = correct[:k].view(-1).float().sum(0, keepdim = True)
        acc_1 = correct_1 / batch_size
        acc_k = correct_k / batch_size
    return acc_1, acc_k


def cluster_vis():
    """visualize point cloud in cluster
    """
    train_dataset = ShapeNet(opt, train=True, num_image_per_object=nviews_dic['train'])
    cluster_list = []
    counter = 0
    sample_size = 4

    for label in label_to_cd_index:
        cd_index = label_to_cd_index[label]
        cluster_sample_indexes = random.choices(label_to_index[label], k=sample_size)


        train_sample = train_dataset[cd_index]
        cd_pts = train_sample['points']
        cd_pts = cd_pts.unsqueeze(0)
        if opt.mode == 'viewer':
            cam_rotmat = train_sample['cam_rotmat'].unsqueeze(0)
            cd_pts = torch.bmm(cd_pts, torch.transpose(cam_rotmat, 1, 2)) 

        for index in cluster_sample_indexes:
            train_sample= train_dataset[index]
            cluster_pts = train_sample['points']
            cluster_pts = cluster_pts.unsqueeze(0)
            if opt.mode == 'viewer':
                cam_rotmat = train_sample['cam_rotmat'].unsqueeze(0)
                cluster_pts = torch.bmm(cluster_pts, torch.transpose(cam_rotmat, 1, 2)) 

            cd_pts = torch.cat((cd_pts, cluster_pts), dim=0)
        cluster_list.append(cd_pts)

        if counter == 7:
            break
        counter += 1
    check_points = torch.cat(cluster_list, dim=0)
    visuaize_pts(check_points, f"K Medoids Cluster Visulaization {opt.mode} Center", rows=8, cols=sample_size + 1, elev=0, azim=0)
    plt.savefig(f"log/visualization/cluster_vis_{opt.mode}.png")


def cluster_distribution_vis(label_to_index):
    """Visualize Cluster Distribution

    Args:
        label_to_index ([type]): [description]
    """
    cluster_size_list = []
    for label in label_to_index:
        cluster_size_list.append(len(label_to_index[label]))
    plt.figure(figsize=(20, 10))
    plt.bar(range(len(cluster_size_list)), cluster_size_list, width=0.8)
    plt.title("K Medoids Cluster Size Distribution")
    plt.savefig("log/visualization/cluster_distribution.png")


def sanity_test_dismat(dismat):
    """
    Check correctness of dismat 
    Output:
        0 [8805, 4135] expect: 0.19319981 actual: 0.19693542
        1 [16716, 7727] expect: 0.10422261 actual: 0.10647938
        2 [32468, 29457] expect: 0.09711268 actual: 0.09769807
        3 [30949, 24878] expect: 0.08413294 actual: 0.08219208
        4 [13759, 6151] expect: 0.13223734 actual: 0.13165328
        5 [31972, 1857] expect: 0.04982451 actual: 0.05099706
    """
    train_dataset = ShapeNet(opt, train=True, num_image_per_object=nviews_dic['train'])
    train_loader = torch.utils.data.DataLoader(train_dataset, 
                                batch_size=1, shuffle=False, num_workers=opt.workers)
    chamfer = ChamferDistanceL2().to(opt.device)
    i = 0
    while i < 20:
        indexes = random.sample(range(len(train_dataset)), 2)
        pts_list = []
        for index in indexes:
            train_sample = train_dataset[index]
            train_points, train_cam_rotmat, = train_sample['points'], train_sample['cam_rotmat']
            train_points = train_points.unsqueeze(0)
            train_cam_rotmat = train_cam_rotmat.unsqueeze(0)     
            train_points = torch.bmm(train_points, torch.transpose(train_cam_rotmat, 1, 2))
            train_points = train_points.to(opt.device)
            pts_list.append(train_points)


        expected = chamfer(pts_list[0], pts_list[1]).cpu().numpy()
        actual = dismat[indexes[0], indexes[1]]
        print(i, indexes, "expect:", expected, "actual:", actual)
        i += 1



if __name__ == '__main__':
    opt = parser.parse_args()
    # Set-up output directories
    print(opt)

    if not opt.test:
        dt = datetime.datetime.now().strftime('%y%m%d_%H%M')
        net_desc = '{}_{}'.format(dt, '_'.join(opt.description.split()))
        save_folder = os.path.join(opt.base_dir, net_desc)
        if not os.path.exists(save_folder):
            os.makedirs(save_folder)
        logger = logging.getLogger()
        file_log_handler = logging.FileHandler(os.path.join(save_folder, "train.log"))
        logger.addHandler(file_log_handler)
        opt.save_folder = save_folder

        opt = EasyDict(opt.__dict__)
        with open(os.path.join(opt.save_folder, 'opts.yaml'), 'w') as file:
            yaml.dump(dict(opt), file, default_flow_style=False, sort_keys=False)

        opt.tbwriter = tensorboardX.SummaryWriter(os.path.join(opt.base_dir, 'tensorboard', net_desc))
    else:
        save_folder = opt.network_dir
        opt.save_folder = save_folder
        logger = logging.getLogger()
        file_log_handler = logging.FileHandler(os.path.join(save_folder, "eval.log"))
        logger.addHandler(file_log_handler)
        

    opt.save_folder = save_folder
    stderr_log_handler = logging.StreamHandler(sys.stdout)
    logger.addHandler(stderr_log_handler)
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter("%(asctime)s;%(levelname)s;%(message)s","%Y-%m-%d %H:%M:%S")
    file_log_handler.setFormatter(formatter)
    stderr_log_handler.setFormatter(formatter)
    opt.logger = logger
    opt.device = torch.device("cuda")

    main(opt)
    
